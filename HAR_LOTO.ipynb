{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Wearables -- Bonsai Tree Classification on stream data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction & Library Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.classification import accuracy_score, recall_score, f1_score\n",
    "import scipy.stats as st\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bonsai.base.regtree import RegTree\n",
    "from bonsai.base.alphatree import AlphaTree\n",
    "from bonsai.base.c45tree import C45Tree\n",
    "from bonsai.base.ginitree import GiniTree\n",
    "from bonsai.base.xgbtree import XGBTree\n",
    "from bonsai.base.friedmantree import FriedmanTree\n",
    "from bonsai.ensemble.randomforests import RandomForests\n",
    "from bonsai.ensemble.paloboost import PaloBoost\n",
    "from bonsai.ensemble.gbm import GBM\n",
    "import copy\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras import optimizers\n",
    "from utils import *\n",
    "from model import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# Setting seed for reproducability\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "%matplotlib inline\n",
    "\n",
    "#import pydot\n",
    "#import graphviz\n",
    "#pydot.find_graphviz = lambda: True\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_file = 'data/LOTO/MHEALTH.npz'\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "tmp = np.load(data_input_file)\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tmp['X']\n",
    "X = X[:, 0, :, :]\n",
    "y = tmp['y']\n",
    "folds = tmp['folds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hancrafted Template 2017 data/LOTO/MHEALTH.npz\n"
     ]
    }
   ],
   "source": [
    "n_class = y.shape[1]\n",
    "y = np.argmax(y, axis=1)\n",
    "print('Hancrafted Template 2017 {}'.format(data_input_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(sample):\n",
    "    feat = []\n",
    "    for col in range(0,sample.shape[1]):\n",
    "        average = np.average(sample[:,col])\n",
    "        feat.append(average)\n",
    "\n",
    "    return feat\n",
    "\n",
    "def SD(sample):\n",
    "    feat = []\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        std = np.std(sample[:, col])\n",
    "        feat.append(std)\n",
    "\n",
    "    return feat\n",
    "\n",
    "def AAD(sample):\n",
    "    feat = []\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        data = sample[:, col]\n",
    "        add = np.mean(np.absolute(data - np.mean(data)))\n",
    "        feat.append(add)\n",
    "\n",
    "    return feat\n",
    "\n",
    "def ARA(sample):\n",
    "    #Average Resultant Acceleration[1]:\n",
    "    # Average of the square roots of the sum of the values of each axis squared âˆš(xi^2 + yi^2+ zi^2) over the ED\n",
    "    feat = []\n",
    "    sum_square = 0\n",
    "    sample = np.power(sample, 2)\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        sum_square = sum_square + sample[:, col]\n",
    "\n",
    "    sample = np.sqrt(sum_square)\n",
    "    average = np.average(sample)\n",
    "    feat.append(average)\n",
    "    return feat\n",
    "\n",
    "def TBP(sample):\n",
    "    from scipy import signal\n",
    "    feat = []\n",
    "    sum_of_time = 0\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        data = sample[:, col]\n",
    "        peaks = signal.find_peaks_cwt(data, np.arange(1,4))\n",
    "\n",
    "        feat.append(peaks)\n",
    "\n",
    "    return feat\n",
    "\n",
    "def feature_extraction(X):\n",
    "    #Extracts the features, as mentioned by Catal et al. 2015\n",
    "    # Average - A,\n",
    "    # Standard Deviation - SD,\n",
    "    # Average Absolute Difference - AAD,\n",
    "    # Average Resultant Acceleration - ARA(1),\n",
    "    # Time Between Peaks - TBP\n",
    "    X_tmp = []\n",
    "    for sample in X:\n",
    "        features = A(copy.copy(sample))\n",
    "        features = np.hstack((features, A(copy.copy(sample))))\n",
    "        features = np.hstack((features, SD(copy.copy(sample))))\n",
    "        features = np.hstack((features, AAD(copy.copy(sample))))\n",
    "        features = np.hstack((features, ARA(copy.copy(sample))))\n",
    "        #features = np.hstack((features, TBP(sample)))\n",
    "        X_tmp.append(features)\n",
    "\n",
    "    X = np.array(X_tmp)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RegTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy[0.8516] Recall[0.8292] F1[0.8282] at fold[0]\n",
      "______________________________________________________\n",
      "Accuracy[0.9066] Recall[0.8922] F1[0.8952] at fold[1]\n",
      "______________________________________________________\n",
      "Accuracy[0.7537] Recall[0.7709] F1[0.7138] at fold[2]\n",
      "______________________________________________________\n",
      "Accuracy[0.7804] Recall[0.8017] F1[0.7484] at fold[3]\n",
      "______________________________________________________\n",
      "Accuracy[0.7757] Recall[0.7862] F1[0.7293] at fold[4]\n",
      "______________________________________________________\n",
      "Accuracy[0.8836] Recall[0.8821] F1[0.8572] at fold[5]\n",
      "______________________________________________________\n",
      "Accuracy[0.8962] Recall[0.9022] F1[0.8772] at fold[6]\n",
      "______________________________________________________\n",
      "Accuracy[0.9607] Recall[0.8949] F1[0.8970] at fold[7]\n",
      "______________________________________________________\n",
      "Accuracy[0.9500] Recall[0.9541] F1[0.9543] at fold[8]\n",
      "______________________________________________________\n",
      "Accuracy[0.8571] Recall[0.8509] F1[0.7918] at fold[9]\n",
      "______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "avg_ttime=[]\n",
    "avg_ptime=[]\n",
    "avg_size=[]\n",
    "method\n",
    "for i in range(0, len(folds)):\n",
    "    train_idx = folds[i][0]\n",
    "    test_idx = folds[i][1]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "        #Your train goes here. For instance:\n",
    "    #X_train=X_train.transpose(0,1,2).reshape(X_train.shape[0],-1)\n",
    "    #X_test=X_test.transpose(0,1,2).reshape(X_test.shape[0],-1)\n",
    "    X_train = feature_extraction(X_train)\n",
    "    X_test = feature_extraction(X_test)      \n",
    "        \n",
    "    method = RegTree(max_depth=8)\n",
    "    t0=time.time()\n",
    "    method.fit(X_train, y_train)\n",
    "    avg_ttime.append(time.time()-t0)\n",
    "            #Your testing goes here. For instance:\n",
    "    t1=time.time()\n",
    "    y_pred = method.predict(X_test)\n",
    "    avg_ptime.append(time.time()-t1)\n",
    "    y_pred=np.round(y_pred,0)\n",
    "    y_pred=y_pred.astype(int)\n",
    "    v=method.dump()\n",
    "    avg_size.append(round(v.__sizeof__()/1024,3))\n",
    "\n",
    "    acc_fold = accuracy_score(y_test, y_pred)\n",
    "    avg_acc.append(acc_fold)\n",
    "\n",
    "    recall_fold = recall_score(y_test, y_pred, average='macro')\n",
    "    avg_recall.append(recall_fold)\n",
    "\n",
    "    f1_fold  = f1_score(y_test, y_pred, average='macro')\n",
    "    avg_f1.append(f1_fold)\n",
    "\n",
    "    print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at fold[{}]'.format(acc_fold, recall_fold, f1_fold ,i))\n",
    "    print('______________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy[0.8616] IC [0.8196, 0.9035]\n",
      "Mean Recall[0.8564] IC [0.8224, 0.8905]\n",
      "Mean F1[0.8292] IC [0.7823, 0.8762]\n",
      "Mean size[0.867]\n",
      "Mean training time[1155.292]\n",
      "Mean prediction time[0.132]\n"
     ]
    }
   ],
   "source": [
    "ic_acc = st.t.interval(0.9, len(avg_acc) - 1, loc=np.mean(avg_acc), scale=st.sem(avg_acc))\n",
    "ic_recall = st.t.interval(0.9, len(avg_recall) - 1, loc=np.mean(avg_recall), scale=st.sem(avg_recall))\n",
    "ic_f1 = st.t.interval(0.9, len(avg_f1) - 1, loc=np.mean(avg_f1), scale=st.sem(avg_f1))\n",
    "print('Mean Accuracy[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_acc), ic_acc[0], ic_acc[1]))\n",
    "print('Mean Recall[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_recall), ic_recall[0], ic_recall[1]))\n",
    "print('Mean F1[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_f1), ic_f1[0], ic_f1[1]))\n",
    "print('Mean size[{:.3f}]'.format(np.mean(avg_size)))\n",
    "print('Mean training time[{:.3f}]'.format(round(np.mean(avg_ttime)*1000,3)))\n",
    "print('Mean prediction time[{:.3f}]'.format(round(np.mean(avg_ptime)*1000,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy[0.8711] Recall[0.8455] F1[0.8487] at fold[0]\n",
      "______________________________________________________\n",
      "Accuracy[0.8405] Recall[0.8384] F1[0.8177] at fold[1]\n",
      "______________________________________________________\n",
      "Accuracy[0.7649] Recall[0.7818] F1[0.7252] at fold[2]\n",
      "______________________________________________________\n",
      "Accuracy[0.8078] Recall[0.8233] F1[0.7687] at fold[3]\n",
      "______________________________________________________\n",
      "Accuracy[0.7757] Recall[0.7862] F1[0.7293] at fold[4]\n",
      "______________________________________________________\n",
      "Accuracy[0.9455] Recall[0.9424] F1[0.9454] at fold[5]\n",
      "______________________________________________________\n",
      "Accuracy[0.8846] Recall[0.8913] F1[0.8664] at fold[6]\n",
      "______________________________________________________\n",
      "Accuracy[0.9694] Recall[0.9664] F1[0.9375] at fold[7]\n",
      "______________________________________________________\n",
      "Accuracy[0.9333] Recall[0.9382] F1[0.8979] at fold[8]\n",
      "______________________________________________________\n",
      "Accuracy[0.8254] Recall[0.8333] F1[0.7794] at fold[9]\n",
      "______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "avg_ttime=[]\n",
    "avg_ptime=[]\n",
    "avg_size=[]\n",
    "for i in range(0, len(folds)):\n",
    "    train_idx = folds[i][0]\n",
    "    test_idx = folds[i][1]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    \n",
    "    X_train = feature_extraction(X_train)\n",
    "    X_test = feature_extraction(X_test)      \n",
    "        \n",
    "    method = XGBTree(max_depth=8,min_samples_split=1,min_samples_leaf=2)\n",
    "    t0=time.time()\n",
    "    method.fit(X_train, y_train)\n",
    "    avg_ttime.append(time.time()-t0)\n",
    "            #Your testing goes here. For instance:\n",
    "    t1=time.time()\n",
    "    y_pred = method.predict(X_test)\n",
    "    avg_ptime.append(time.time()-t1)\n",
    "    y_pred=np.round(y_pred,0)\n",
    "    y_pred=y_pred.astype(int)\n",
    "    v=method.dump()\n",
    "    avg_size.append(round(v.__sizeof__()/1024,3))\n",
    "\n",
    "    acc_fold = accuracy_score(y_test, y_pred)\n",
    "    avg_acc.append(acc_fold)\n",
    "\n",
    "    recall_fold = recall_score(y_test, y_pred, average='macro')\n",
    "    avg_recall.append(recall_fold)\n",
    "\n",
    "    f1_fold  = f1_score(y_test, y_pred, average='macro')\n",
    "    avg_f1.append(f1_fold)\n",
    "\n",
    "    print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at fold[{}]'.format(acc_fold, recall_fold, f1_fold ,i))\n",
    "    print('______________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy[0.8618] IC [0.8205, 0.9032]\n",
      "Mean Recall[0.8647] IC [0.8264, 0.9030]\n",
      "Mean F1[0.8316] IC [0.7848, 0.8784]\n",
      "Mean size[0.640]\n",
      "Mean training time[960.636]\n",
      "Mean prediction time[0.164]\n"
     ]
    }
   ],
   "source": [
    "ic_acc = st.t.interval(0.9, len(avg_acc) - 1, loc=np.mean(avg_acc), scale=st.sem(avg_acc))\n",
    "ic_recall = st.t.interval(0.9, len(avg_recall) - 1, loc=np.mean(avg_recall), scale=st.sem(avg_recall))\n",
    "ic_f1 = st.t.interval(0.9, len(avg_f1) - 1, loc=np.mean(avg_f1), scale=st.sem(avg_f1))\n",
    "print('Mean Accuracy[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_acc), ic_acc[0], ic_acc[1]))\n",
    "print('Mean Recall[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_recall), ic_recall[0], ic_recall[1]))\n",
    "print('Mean F1[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_f1), ic_f1[0], ic_f1[1]))\n",
    "print('Mean size[{:.3f}]'.format(np.mean(avg_size)))\n",
    "print('Mean training time[{:.3f}]'.format(round(np.mean(avg_ttime)*1000,3)))\n",
    "print('Mean prediction time[{:.3f}]'.format(round(np.mean(avg_ptime)*1000,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FriedmanTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy[0.8555] Recall[0.8312] F1[0.8326] at fold[0]\n",
      "______________________________________________________\n",
      "Accuracy[0.9494] Recall[0.9534] F1[0.9541] at fold[1]\n",
      "______________________________________________________\n",
      "Accuracy[0.6754] Recall[0.6952] F1[0.6091] at fold[2]\n",
      "______________________________________________________\n",
      "Accuracy[0.8039] Recall[0.8197] F1[0.7656] at fold[3]\n",
      "______________________________________________________\n",
      "Accuracy[0.7757] Recall[0.7862] F1[0.7293] at fold[4]\n",
      "______________________________________________________\n",
      "Accuracy[0.8909] Recall[0.8983] F1[0.8685] at fold[5]\n",
      "______________________________________________________\n",
      "Accuracy[0.8808] Recall[0.8891] F1[0.8640] at fold[6]\n",
      "______________________________________________________\n",
      "Accuracy[0.9738] Recall[0.9783] F1[0.9458] at fold[7]\n",
      "______________________________________________________\n",
      "Accuracy[0.9500] Recall[0.9541] F1[0.9307] at fold[8]\n",
      "______________________________________________________\n",
      "Accuracy[0.8056] Recall[0.7963] F1[0.7736] at fold[9]\n",
      "______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "avg_ttime=[]\n",
    "avg_ptime=[]\n",
    "avg_size=[]\n",
    "for i in range(0, len(folds)):\n",
    "    train_idx = folds[i][0]\n",
    "    test_idx = folds[i][1]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "        #Your train goes here. For instance:\n",
    "    #X_train=X_train.transpose(0,1,2).reshape(X_train.shape[0],-1)\n",
    "    #X_test=X_test.transpose(0,1,2).reshape(X_test.shape[0],-1)\n",
    "    X_train = feature_extraction(X_train)\n",
    "    X_test = feature_extraction(X_test)      \n",
    "        \n",
    "    method = FriedmanTree(max_depth=10,min_samples_split=1,min_samples_leaf=1)\n",
    "    t0=time.time()\n",
    "    method.fit(X_train, y_train)\n",
    "    avg_ttime.append(time.time()-t0)\n",
    "            #Your testing goes here. For instance:\n",
    "    t1=time.time()\n",
    "    y_pred = method.predict(X_test)\n",
    "    avg_ptime.append(time.time()-t1)\n",
    "    y_pred=np.round(y_pred,0)\n",
    "    y_pred=y_pred.astype(int)\n",
    "    v=method.dump()\n",
    "    avg_size.append(round(v.__sizeof__()/1024,3))\n",
    "\n",
    "    acc_fold = accuracy_score(y_test, y_pred)\n",
    "    avg_acc.append(acc_fold)\n",
    "\n",
    "    recall_fold = recall_score(y_test, y_pred, average='macro')\n",
    "    avg_recall.append(recall_fold)\n",
    "\n",
    "    f1_fold  = f1_score(y_test, y_pred, average='macro')\n",
    "    avg_f1.append(f1_fold)\n",
    "\n",
    "    print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at fold[{}]'.format(acc_fold, recall_fold, f1_fold ,i))\n",
    "    print('______________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy[0.8561] IC [0.8023, 0.9099]\n",
      "Mean Recall[0.8602] IC [0.8080, 0.9124]\n",
      "Mean F1[0.8273] IC [0.7639, 0.8908]\n",
      "Mean size[1.234]\n",
      "Mean training time[1590.875]\n",
      "Mean prediction time[0.137]\n"
     ]
    }
   ],
   "source": [
    "ic_acc = st.t.interval(0.9, len(avg_acc) - 1, loc=np.mean(avg_acc), scale=st.sem(avg_acc))\n",
    "ic_recall = st.t.interval(0.9, len(avg_recall) - 1, loc=np.mean(avg_recall), scale=st.sem(avg_recall))\n",
    "ic_f1 = st.t.interval(0.9, len(avg_f1) - 1, loc=np.mean(avg_f1), scale=st.sem(avg_f1))\n",
    "print('Mean Accuracy[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_acc), ic_acc[0], ic_acc[1]))\n",
    "print('Mean Recall[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_recall), ic_recall[0], ic_recall[1]))\n",
    "print('Mean F1[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_f1), ic_f1[0], ic_f1[1]))\n",
    "print('Mean size[{:.3f}]'.format(np.mean(avg_size)))\n",
    "print('Mean training time[{:.3f}]'.format(round(np.mean(avg_ttime)*1000,3)))\n",
    "print('Mean prediction time[{:.3f}]'.format(round(np.mean(avg_ptime)*1000,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PaloBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy[0.8438] Recall[0.7948] F1[0.7756] at fold[0]\n",
      "______________________________________________________\n",
      "Accuracy[0.8560] Recall[0.8627] F1[0.8178] at fold[1]\n",
      "______________________________________________________\n",
      "Accuracy[0.8209] Recall[0.8332] F1[0.7941] at fold[2]\n",
      "______________________________________________________\n",
      "Accuracy[0.8588] Recall[0.8742] F1[0.8402] at fold[3]\n",
      "______________________________________________________\n",
      "Accuracy[0.8745] Recall[0.8804] F1[0.8639] at fold[4]\n",
      "______________________________________________________\n",
      "Accuracy[0.8400] Recall[0.8393] F1[0.7965] at fold[5]\n",
      "______________________________________________________\n",
      "Accuracy[0.9038] Recall[0.9094] F1[0.8970] at fold[6]\n",
      "______________________________________________________\n",
      "Accuracy[0.8996] Recall[0.8739] F1[0.8677] at fold[7]\n",
      "______________________________________________________\n",
      "Accuracy[0.8708] Recall[0.8335] F1[0.8134] at fold[8]\n",
      "______________________________________________________\n",
      "Accuracy[0.7659] Recall[0.7080] F1[0.6688] at fold[9]\n",
      "______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "avg_ttime=[]\n",
    "avg_ptime=[]\n",
    "avg_size=[]\n",
    "for i in range(0, len(folds)):\n",
    "    train_idx = folds[i][0]\n",
    "    test_idx = folds[i][1]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "        #Your train goes here. For instance:\n",
    "    #X_train=X_train.transpose(0,1,2).reshape(X_train.shape[0],-1)\n",
    "    #X_test=X_test.transpose(0,1,2).reshape(X_test.shape[0],-1)\n",
    "    X_train = feature_extraction(X_train)\n",
    "    X_test = feature_extraction(X_test)      \n",
    "        \n",
    "    method = PaloBoost(n_estimators=100,max_depth=5)\n",
    "    t0=time.time()\n",
    "    method.fit(X_train, y_train)\n",
    "    avg_ttime.append(time.time()-t0)\n",
    "            #Your testing goes here. For instance:\n",
    "    t1=time.time()\n",
    "    y_pred = method.predict(X_test)\n",
    "    avg_ptime.append(time.time()-t1)\n",
    "    y_pred=np.round(y_pred,0)\n",
    "    y_pred=y_pred.astype(int)\n",
    "    v=method.dump()\n",
    "    avg_size.append(round(v.__sizeof__()/1024,3))\n",
    "\n",
    "    acc_fold = accuracy_score(y_test, y_pred)\n",
    "    avg_acc.append(acc_fold)\n",
    "\n",
    "    recall_fold = recall_score(y_test, y_pred, average='macro')\n",
    "    avg_recall.append(recall_fold)\n",
    "\n",
    "    f1_fold  = f1_score(y_test, y_pred, average='macro')\n",
    "    avg_f1.append(f1_fold)\n",
    "\n",
    "    print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at fold[{}]'.format(acc_fold, recall_fold, f1_fold ,i))\n",
    "    print('______________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy[0.8534] IC [0.8302, 0.8766]\n",
      "Mean Recall[0.8409] IC [0.8081, 0.8737]\n",
      "Mean F1[0.8135] IC [0.7767, 0.8503]\n",
      "Mean size[0.211]\n",
      "Mean training time[22184.167]\n",
      "Mean prediction time[7.539]\n"
     ]
    }
   ],
   "source": [
    "ic_acc = st.t.interval(0.9, len(avg_acc) - 1, loc=np.mean(avg_acc), scale=st.sem(avg_acc))\n",
    "ic_recall = st.t.interval(0.9, len(avg_recall) - 1, loc=np.mean(avg_recall), scale=st.sem(avg_recall))\n",
    "ic_f1 = st.t.interval(0.9, len(avg_f1) - 1, loc=np.mean(avg_f1), scale=st.sem(avg_f1))\n",
    "print('Mean Accuracy[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_acc), ic_acc[0], ic_acc[1]))\n",
    "print('Mean Recall[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_recall), ic_recall[0], ic_recall[1]))\n",
    "print('Mean F1[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_f1), ic_f1[0], ic_f1[1]))\n",
    "print('Mean size[{:.3f}]'.format(np.mean(avg_size)))\n",
    "print('Mean training time[{:.3f}]'.format(round(np.mean(avg_ttime)*1000,3)))\n",
    "print('Mean prediction time[{:.3f}]'.format(round(np.mean(avg_ptime)*1000,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy[0.8516] Recall[0.8296] F1[0.8101] at fold[0]\n",
      "______________________________________________________\n",
      "Accuracy[0.8872] Recall[0.8954] F1[0.8705] at fold[1]\n",
      "______________________________________________________\n",
      "Accuracy[0.7948] Recall[0.8103] F1[0.7579] at fold[2]\n",
      "______________________________________________________\n",
      "Accuracy[0.8000] Recall[0.8181] F1[0.7750] at fold[3]\n",
      "______________________________________________________\n",
      "Accuracy[0.8859] Recall[0.8913] F1[0.8807] at fold[4]\n",
      "______________________________________________________\n",
      "Accuracy[0.9527] Recall[0.9681] F1[0.9588] at fold[5]\n",
      "______________________________________________________\n",
      "Accuracy[0.9346] Recall[0.9384] F1[0.9356] at fold[6]\n",
      "______________________________________________________\n",
      "Accuracy[0.9083] Recall[0.8901] F1[0.8887] at fold[7]\n",
      "______________________________________________________\n",
      "Accuracy[0.9417] Recall[0.8992] F1[0.8988] at fold[8]\n",
      "______________________________________________________\n",
      "Accuracy[0.6746] Recall[0.6255] F1[0.5654] at fold[9]\n",
      "______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "avg_ttime=[]\n",
    "avg_ptime=[]\n",
    "avg_size=[]\n",
    "for i in range(0, len(folds)):\n",
    "    train_idx = folds[i][0]\n",
    "    test_idx = folds[i][1]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "        #Your train goes here. For instance:\n",
    "    #X_train=X_train.transpose(0,1,2).reshape(X_train.shape[0],-1)\n",
    "    #X_test=X_test.transpose(0,1,2).reshape(X_test.shape[0],-1)\n",
    "    X_train = feature_extraction(X_train)\n",
    "    X_test = feature_extraction(X_test)      \n",
    "        \n",
    "    method = GBM(n_estimators=100,max_depth=5)\n",
    "    t0=time.time()\n",
    "    method.fit(X_train, y_train)\n",
    "    avg_ttime.append(time.time()-t0)\n",
    "            #Your testing goes here. For instance:\n",
    "    t1=time.time()\n",
    "    y_pred = method.predict(X_test)\n",
    "    avg_ptime.append(time.time()-t1)\n",
    "    y_pred=np.round(y_pred,0)\n",
    "    y_pred=y_pred.astype(int)\n",
    "    v=method.dump()\n",
    "    avg_size.append(round(v.__sizeof__()/1024,3))\n",
    "\n",
    "    acc_fold = accuracy_score(y_test, y_pred)\n",
    "    avg_acc.append(acc_fold)\n",
    "\n",
    "    recall_fold = recall_score(y_test, y_pred, average='macro')\n",
    "    avg_recall.append(recall_fold)\n",
    "\n",
    "    f1_fold  = f1_score(y_test, y_pred, average='macro')\n",
    "    avg_f1.append(f1_fold)\n",
    "\n",
    "    print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at fold[{}]'.format(acc_fold, recall_fold, f1_fold ,i))\n",
    "    print('______________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy[0.8631] IC [0.8132, 0.9131]\n",
      "Mean Recall[0.8566] IC [0.8011, 0.9121]\n",
      "Mean F1[0.8342] IC [0.7676, 0.9007]\n",
      "Mean size[0.211]\n",
      "Mean training time[85688.399]\n",
      "Mean prediction time[8.123]\n"
     ]
    }
   ],
   "source": [
    "ic_acc = st.t.interval(0.9, len(avg_acc) - 1, loc=np.mean(avg_acc), scale=st.sem(avg_acc))\n",
    "ic_recall = st.t.interval(0.9, len(avg_recall) - 1, loc=np.mean(avg_recall), scale=st.sem(avg_recall))\n",
    "ic_f1 = st.t.interval(0.9, len(avg_f1) - 1, loc=np.mean(avg_f1), scale=st.sem(avg_f1))\n",
    "print('Mean Accuracy[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_acc), ic_acc[0], ic_acc[1]))\n",
    "print('Mean Recall[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_recall), ic_recall[0], ic_recall[1]))\n",
    "print('Mean F1[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_f1), ic_f1[0], ic_f1[1]))\n",
    "print('Mean size[{:.3f}]'.format(np.mean(avg_size)))\n",
    "print('Mean training time[{:.3f}]'.format(round(np.mean(avg_ttime)*1000,3)))\n",
    "print('Mean prediction time[{:.3f}]'.format(round(np.mean(avg_ptime)*1000,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable    \n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Model\", \"Mean Accuracy\", \"Mean Recall\", \"Mean F1\"]\n",
    "x.add_row([\"Reg Tree\", 0.8616,0.8564,0.8292])\n",
    "x.add_row([\"XGB Tree\", 0.8618, 0.8647, 0.8316])\n",
    "x.add_row([\"Friedman Tree\", 0.8561, 0.8602, 0.8273])\n",
    "x.add_row([\"Palo Boost\", 0.8534, 0.8409, 0.8135])\n",
    "x.add_row([\"GBM\", 0.8631, 0.8566, 0.8342])\n",
    "\n",
    "y = PrettyTable()\n",
    "y.field_names = [\"Model\", \"Mean Accuracy\", \"Mean Recall\", \"Mean F1\"]\n",
    "y.add_row([\"Mean\", 85.92, 0, 0])\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
